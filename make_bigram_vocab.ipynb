{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference From https://github.com/wlin12/wang2vec/blob/master/word2phrase.c\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import json\n",
    "\n",
    "vocab_hash_size = 100000000 #100M for bigram\n",
    "vocab_hash = [int(-1)] * vocab_hash_size\n",
    "\n",
    "train_file = None\n",
    "\n",
    "vocab = []\n",
    "vocab_size = 0\n",
    "\n",
    "min_count = 5\n",
    "min_reduce = 1\n",
    "unk_num = 0\n",
    "\n",
    "train_words = 0\n",
    "\n",
    "\n",
    "# Returns hash value of a word\n",
    "def GetWordHash(word) :\n",
    "    \n",
    "    hash = 1\n",
    "    for a in range(0, len(word)) :\n",
    "        hash = hash * 257 + ord(word[a])\n",
    "    hash = hash % vocab_hash_size\n",
    "    return hash\n",
    "\n",
    "\n",
    "# Returns position of a word in the vocabulary; if the word is not found, returns -1\n",
    "def SearchVocab(word) :\n",
    "    hash = GetWordHash(word)\n",
    "    while True : \n",
    "        if vocab_hash[hash] == -1 :\n",
    "            return -1\n",
    "        if word == vocab[vocab_hash[hash]]['word'] :\n",
    "            return vocab_hash[hash]\n",
    "        hash = (hash + 1) % vocab_hash_size\n",
    "    \n",
    "    return -1\n",
    "\n",
    "\n",
    "# Adds a word to the vocabulary\n",
    "def AddWordToVocab(word) :\n",
    "    global vocab_size, vocab, vocab_hash_size, vocab_hash\n",
    "    \n",
    "    # 이 부분이 C 에비해 심각하게 시간이 오래걸릴것으로 예상...\n",
    "    vocab.append({\"cn\" : 0, \"word\" : word})\n",
    "    vocab_size += 1\n",
    "    \n",
    "    hash = GetWordHash(word)\n",
    "    \n",
    "    while vocab_hash[hash] != -1 :\n",
    "        hash = (hash + 1) % vocab_hash_size\n",
    "    \n",
    "    vocab_hash[hash] = vocab_size - 1\n",
    "    \n",
    "    # return Word index\n",
    "    return vocab_size - 1\n",
    "        \n",
    "\n",
    "# Sorts the vocabulary by frequency using word counts\n",
    "def SortVocab() :\n",
    "    global vocab_size, vocab, vocab_hash_size, vocab_hash, min_count, unk_num\n",
    "    \n",
    "    # Sort the vocabulary and keep </s> at the first position\n",
    "    vocab = sorted(vocab, key=lambda k : k['cn'], reverse=True)\n",
    "    \n",
    "    for a in range(0, vocab_hash_size) :\n",
    "        vocab_hash[a] = -1\n",
    "        \n",
    "        \n",
    "    b = vocab_size\n",
    "    for a in range(0, vocab_size) : \n",
    "        # Words occuring less than min_count times will be discarded from the vocab\n",
    "        if vocab[a][\"cn\"] < min_count :\n",
    "            b = a\n",
    "            break\n",
    "        else :\n",
    "            # Hash will be re-computed as after the sorting it is not actual\n",
    "            hash = GetWordHash(vocab[a]['word'])\n",
    "            while vocab_hash[hash] != -1 :\n",
    "                hash = (hash + 1) % vocab_hash_size\n",
    "                vocab_hash[hash] = a\n",
    "                \n",
    "    for a in range(b, vocab_size) :\n",
    "        # deletion like tetris\n",
    "        del vocab[b]\n",
    "        unk_num += 1\n",
    "    \n",
    "    vocab_size = b\n",
    "\n",
    "\n",
    "# Reduces the vocabulary by removing infrequent tokens\n",
    "def ReduceVocab() :\n",
    "    global vocab_size, vocab, vocab_hash_size, vocab_hash, min_reduce, unk_num\n",
    "    \n",
    "    b = 0\n",
    "    for a in range(0, vocab_size) :\n",
    "        if vocab[a][\"cn\"] > min_reduce :\n",
    "            vocab[b][\"cn\"] = vocab[a][\"cn\"]\n",
    "            vocab[b]['word'] = vocab[a]['word']\n",
    "            b += 1\n",
    "    \n",
    "    # delete reduced vocab\n",
    "    for a in range(b, vocab_size) :\n",
    "        # deletion like tetris\n",
    "        del vocab[b]\n",
    "        unk_num += 1\n",
    "    \n",
    "    vocab_size = b\n",
    "    for a in range(0, vocab_hash_size) :\n",
    "        vocab_hash[a] = -1\n",
    "    \n",
    "    for a in range(0, vocab_size) :\n",
    "        # Hash will be re-computed as it is not actual\n",
    "        hash = GetWordHash(vocab[a]['word'])\n",
    "        while vocab_hash[hash] != -1 :\n",
    "            hash = (hash + 1) % vocab_hash_size\n",
    "        vocab_hash[hash] = a\n",
    "    \n",
    "    min_reduce += 1\n",
    "    \n",
    "\n",
    "def LearnVocabFromTrainFile() :\n",
    "    global vocab_size, vocab, vocab_hash_size, vocab_hash, train_words, train_file\n",
    "    \n",
    "    word = \"\"\n",
    "    last_word = \"\"\n",
    "    bigram_word = \"\"\n",
    "    \n",
    "    fin = open(train_file, \"r\")\n",
    "    \n",
    "    vocab_size = 0\n",
    "    \n",
    "    for line in fin.readlines() :\n",
    "        words = line.split()\n",
    "        \n",
    "        start = 1\n",
    "        \n",
    "        for word in words : \n",
    "            word = word.strip()\n",
    "\n",
    "            train_words += 1\n",
    "\n",
    "            if train_words % 100000 == 0 :\n",
    "                print(\"Words processed : %dK        Vocab Size : %dK\" % (train_words/1000, vocab_size / 1000), flush=True)\n",
    "\n",
    "            i = SearchVocab(word)\n",
    "            if i == -1 :\n",
    "                a = AddWordToVocab(word)\n",
    "                vocab[a][\"cn\"] = 1\n",
    "            else :\n",
    "                vocab[i][\"cn\"] += 1\n",
    "\n",
    "            if start == 1 :\n",
    "                last_word = word\n",
    "                start = 0\n",
    "                continue\n",
    "\n",
    "            bigram_word = last_word + \"_\" + word\n",
    "            last_word = word\n",
    "\n",
    "            i = SearchVocab(bigram_word)\n",
    "            if i == -1 :\n",
    "                a = AddWordToVocab(bigram_word)\n",
    "                vocab[a][\"cn\"] = 1\n",
    "            else :\n",
    "                vocab[i][\"cn\"] += 1\n",
    "\n",
    "            # for good hashing\n",
    "            if vocab_size > vocab_hash_size * 0.7 :\n",
    "                ReduceVocab()\n",
    "            \n",
    "    SortVocab()\n",
    "    a = AddWordToVocab(\"<unk>\")\n",
    "    vocab[a]['cn'] = unk_num\n",
    "    print(\"\\nVocab size (unigrams + bigrams): %d\\n\" % vocab_size, flush=True)\n",
    "    print(\"Words in train file: %d\\n\" % train_words, flush=True)\n",
    "    \n",
    "    fin.close()\n",
    "    \n",
    "\n",
    "def TrainModel() :\n",
    "    global train_file\n",
    "    \n",
    "    print(\"Starting training using file %s\\n\" % train_file)\n",
    "    LearnVocabFromTrainFile()\n",
    "    \n",
    "    global vocab, vocab_hash\n",
    "    \n",
    "    with open('./vocab/vocab.json', 'w', encoding='utf-8') as make_file:\n",
    "        json.dump(vocab, make_file, indent=\"\\t\")\n",
    "        \n",
    "    with open('./vocab/vocab_hash.json', 'w', encoding='utf-8') as make_file:\n",
    "        json.dump(vocab_hash, make_file, indent=\"\\t\")\n",
    "\n",
    "def main() :\n",
    "    global train_file\n",
    "    \n",
    "    train_file = \"./train.txt\"\n",
    "    \n",
    "    TrainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train.txt\n",
      "Starting training using file ./train.txt\n",
      "\n",
      "Words processed : 100K        Vocab Size : 77K\n",
      "Words processed : 200K        Vocab Size : 133K\n",
      "Words processed : 300K        Vocab Size : 184K\n",
      "\n",
      "Vocab size (unigrams + bigrams): 14500\n",
      "\n",
      "Words in train file: 331835\n",
      "\n",
      "completed time : 61.35 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "current_time = time.time()\n",
    "\n",
    "main()\n",
    "\n",
    "print(\"completed time : %.2f seconds\" % (time.time() - current_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cn': 11243, 'word': 'the'},\n",
       " {'cn': 9970, 'word': ','},\n",
       " {'cn': 8663, 'word': '.'},\n",
       " {'cn': 7472, 'word': 'to'},\n",
       " {'cn': 6263, 'word': 'a'},\n",
       " {'cn': 6126, 'word': 'of'},\n",
       " {'cn': 5644, 'word': 'in'},\n",
       " {'cn': 5311, 'word': ';'},\n",
       " {'cn': 4317, 'word': 'and'},\n",
       " {'cn': 3466, 'word': 'on'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./vocab/vocab.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "    \n",
    "json_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
