{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_train_data_save(df, output_file=\"./train.txt\") :\n",
    "    string = \"\"\n",
    "\n",
    "    for i in range(0, len(df)) :\n",
    "        string += \" \".join(word_tokenize(df['Title'][i])) + \"\\n\" + \" \".join(word_tokenize(df['Content'][i])) + \"\\n\"\n",
    "\n",
    "    with open(output_file, \"w\") as f :\n",
    "        f.write(string)\n",
    "    \n",
    "    del string\n",
    "    \n",
    "# it will used to make huffman tree \n",
    "def class_count_save(df, output_file=\"./data/class_vocab.json\") :\n",
    "    \n",
    "    class_count = []\n",
    "    \n",
    "    for i in range(0, len(df)) :\n",
    "        cls = int(df['Class'][i])\n",
    "        \n",
    "        while cls >= len(class_count) :\n",
    "            class_count.append({\n",
    "                'cn' : 0,\n",
    "                'class' : len(class_count)\n",
    "            })\n",
    "            \n",
    "        class_count[cls]['cn'] += 1\n",
    "    \n",
    "    # what we need is just rank\n",
    "    sorted_cls = [k['cn'] for k in class_count]\n",
    "    sorted_cls = sorted(sorted_cls)\n",
    "    \n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as make_file:\n",
    "        json.dump([sorted_cls.index(k['cn']) for k in class_count if k['cn'] > 0], make_file, indent=\"\\t\")\n",
    "    \n",
    "    return sorted_cls\n",
    "    \n",
    "    \n",
    "def make_document(df) :\n",
    "    data = []\n",
    "    \n",
    "    # bag of words \n",
    "    for i in range(0, len(df)) : \n",
    "        data.append({\n",
    "            \"class\" : int(df['Class'][i]),\n",
    "            \"document\" : word_tokenize(str(df['Title'][i])) + word_tokenize(str(df['Content'][i]))\n",
    "        })\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetWordHash(word, vocab_hash) :\n",
    "    hash = 1\n",
    "    for a in range(0, len(word)) :\n",
    "        hash = hash * 257 + ord(word[a])\n",
    "    hash = hash % len(vocab_hash)\n",
    "    \n",
    "    return hash\n",
    "\n",
    "\n",
    "def SearchVocab(word, vocab, vocab_hash) :\n",
    "    hash = GetWordHash(word, vocab_hash)\n",
    "    \n",
    "    while True : \n",
    "        if vocab_hash[hash] == - 1 :\n",
    "            return -1\n",
    "        if word == vocab[vocab_hash[hash]].word :\n",
    "            return vocab_hash[hash]\n",
    "        \n",
    "        hash = (hash + 1) % vocab_hash_size\n",
    "    \n",
    "    return -1\n",
    "\n",
    "\n",
    "def add_bigram_features(data, vocab, vocab_hash) :\n",
    "    \n",
    "    for i in range(0, len(data)) :\n",
    "        \n",
    "        data[i]['bigram_features'] = []\n",
    "        \n",
    "        last_word = \"\"\n",
    "        \n",
    "        for j in range(0,len(data[i]['document'])) :\n",
    "            word = data[i]['document'][j]\n",
    "            bigram_word = last_word + \"_\" + word\n",
    "            \n",
    "            idx = SearchVocab(bigram_word, vocab, vocab_hash)\n",
    "            if idx != -1 :\n",
    "                data[i]['bigram_features'].append(bigram_word)\n",
    "            \n",
    "            last_word = word\n",
    "            \n",
    "            if SearchVocab(word, vocab, vocab_hash) == -1 :\n",
    "                data[i]['document'][j] = \"<unk>\"\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() :\n",
    "    df = pd.read_csv(\"./data/train.csv\", header=None)\n",
    "    df.columns = [\"Class\", \"Title\", \"Content\"]\n",
    "    \n",
    "    data = make_document(df)\n",
    "    \n",
    "    del df\n",
    "    \n",
    "    vocab_file=\"./vocab/vocab.json\"\n",
    "    vocab_hash_file=\"./vocab/vocab_hash.json\"\n",
    "    \n",
    "    with open(vocab_file, 'r') as f:\n",
    "        vocab= json.load(f)\n",
    "    with open(vocab_hash_file, 'r') as f:\n",
    "        vocab_hash = json.load(f)\n",
    "    \n",
    "    data = add_bigram_features(data, vocab, vocab_hash)\n",
    "    \n",
    "    with open('./data/processed_data.json', 'w', encoding='utf-8') as make_file:\n",
    "        json.dump(data, make_file, indent=\"\\t\")\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/train.csv\", header=None)\n",
    "df.columns = [\"Class\", \"Title\", \"Content\"]\n",
    "sorted_cls = class_count_save(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/class_vocab.json\" ,\"r\") as f :\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 30000, 30000, 30000, 30000]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_cls[0:len(sorted_cls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
